import os
import logging
import math
import torch
import torch.distributed as dist
import datetime as dt
from typing import Union
import numpy as np

import math
import torch
import warnings


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
    tensor: an n-dimensional `torch.Tensor`
    mean: the mean of the normal distribution
    std: the standard deviation of the normal distribution
    a: the minimum cutoff value
    b: the maximum cutoff value
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)





# dummy placeholders
_COMM_LIST = []
_COMM_NAMES = {}

# world comm
class comm:
    def get_size(comm_id: Union[str, int]) -> int:  # pragma: no cover
        """Returns the size of a specified communicator."""
        if isinstance(comm_id, int):
            cid = comm_id
        else:
            cid = _COMM_NAMES[comm_id] if (comm_id in _COMM_NAMES) else len(_COMM_LIST)

        if not dist.is_initialized() or (cid >= len(_COMM_LIST)):
            return 1
        else:
            return dist.get_world_size(group=_COMM_LIST[cid])


    def get_rank(comm_id: Union[str, int]) -> int:  # pragma: no cover
        """Returns the rank of a specified communicator."""
        if isinstance(comm_id, int):
            cid = comm_id
        else:
            cid = _COMM_NAMES[comm_id] if (comm_id in _COMM_NAMES) else len(_COMM_LIST)

        if not dist.is_initialized() or (cid >= len(_COMM_LIST)):
            return 0
        else:
            return dist.get_rank(group=_COMM_LIST[cid])


    def get_group(comm_id: Union[str, int]) -> int:  # pragma: no cover
        """Returns the group of a specified communicator."""
        if isinstance(comm_id, int):
            cid = comm_id
        else:
            cid = _COMM_NAMES[comm_id] if (comm_id in _COMM_NAMES) else len(_COMM_LIST)

        if not dist.is_initialized() or (cid >= len(_COMM_LIST)):
            raise IndexError(f"Error, comm with id {comm_id} not available.")
        else:
            return _COMM_LIST[cid]


    # specialized routines for world comms
    def get_world_size():  # pragma: no cover
        """Returns the world size"""
        if not dist.is_initialized():
            return 1
        else:
            return dist.get_world_size()


    def get_world_rank():  # pragma: no cover
        """Returns the world rank"""
        if not dist.is_initialized():
            return 0
        else:
            return dist.get_rank()


    def get_local_rank():  # pragma: no cover
        """Returns the local rank of the current process."""
        if os.getenv("LOCAL_RANK") is not None:
            # Use PyTorch env var if available
            return int(os.getenv("LOCAL_RANK"))

        if not dist.is_initialized():
            return 0
        else:
            return get_world_rank() % torch.cuda.device_count()


    def get_names():  # pragma: no cover
        """Returns the names of all available communicators."""
        return _COMM_NAMES


    def is_distributed(name: str):  # pragma: no cover
        """check if distributed."""
        return name in _COMM_NAMES


    # get
    def init(params, verbose=False):  # pragma: no cover
        """Initialize distributed training."""
        # set up global and local communicator
        if params.wireup_info == "env":
            world_size = int(os.getenv("WORLD_SIZE", 1))
            world_rank = int(os.getenv("RANK", 0))
            if os.getenv("WORLD_RANK") is not None:
                # Use WORLD_RANK if available for backwards compatibility
                world_rank = int(os.getenv("WORLD_RANK"))
            port = int(os.getenv("MASTER_PORT", 0))
            master_address = os.getenv("MASTER_ADDR")
            if os.getenv("MASTER_ADDRESS") is not None:
                # Use MASTER_ADDRESS if available for backwards compatibility
                master_address = int(os.getenv("MASTER_ADDRESS"))
        elif params.wireup_info == "mpi":
            import socket
            from mpi4py import MPI

            mpi_comm = MPI.COMM_WORLD.Dup()
            world_size = mpi_comm.Get_size()
            world_rank = mpi_comm.Get_rank()
            my_host = socket.gethostname()
            port = 29500
            master_address = None
            if world_rank == 0:
                master_address_info = socket.getaddrinfo(
                    my_host, port, family=socket.AF_INET, proto=socket.IPPROTO_TCP
                )
                master_address = master_address_info[0][-1][0]
            master_address = mpi_comm.bcast(master_address, root=0)
            os.environ["MASTER_ADDRESS"] = master_address
            os.environ["MASTER_PORT"] = str(port)
        else:
            raise ValueError(f"Error, wireup-info {params.wireup_info} not supported")
        # set local rank to 0 if env var not available
        local_rank = int(os.getenv("LOCAL_RANK", 0))

        if world_size > 1:
            with disable_logging():
                if params.wireup_store == "file":
                    wireup_file_path = os.getenv("WIREUP_FILE_PATH")
                    wireup_store = dist.FileStore(wireup_file_path, world_size)
                elif params.wireup_store == "tcp":
                    # create tcp store
                    wireup_store = dist.TCPStore(
                        host_name=master_address,
                        port=port,
                        world_size=world_size,
                        is_master=(world_rank == 0),
                        timeout=dt.timedelta(seconds=900),
                    )
                else:
                    wireup_store = None

                # initialize process groups
                dist.init_process_group(
                    backend="nccl",
                    rank=world_rank,
                    world_size=world_size,
                    store=wireup_store,
                )

                # get sizes
                world_size = get_world_size()
                world_rank = get_world_rank()
                local_rank = get_local_rank()

                # barrier
                dist.barrier(device_ids=[local_rank])

        # do individual wireup for model parallel comms:
        if hasattr(params, "model_parallel_sizes"):
            model_parallel_sizes = params.model_parallel_sizes
        else:
            model_parallel_sizes = [1]

        if hasattr(params, "model_parallel_names"):
            model_parallel_names = params.model_parallel_names
        else:
            model_parallel_names = ["model"]
        assert len(model_parallel_names) == len(
            model_parallel_sizes
        ), "Please specify names for your communicators"
        model_parallel_size = math.prod(model_parallel_sizes)
        params["model_parallel_size"] = model_parallel_size

        assert (
            world_size % model_parallel_size == 0
        ), "Error, please make sure that the product of model parallel ranks evenly divides the total number of ranks"

        # we set this to be orthogonal to the MP groups
        # we can play tricks with the ddp_group later, in case if all the weights are shared
        data_parallel_size = world_size // model_parallel_size

        # create orthogonal communicators first
        global _COMM_LIST
        global _COMM_NAMES
        if params.log_to_screen:
            logging.info("Starting Wireup")

        if world_size > 1:

            # set up the strides:
            model_parallel_sizes_reversed = model_parallel_sizes[::-1]
            model_grid = np.reshape(
                np.arange(0, model_parallel_size), model_parallel_sizes[::-1]
            )
            perm = np.roll(np.arange(0, len(model_parallel_sizes)), 1).tolist()
            ranks_lookup = {}

            comm_count = 0
            for mpname in model_parallel_names:
                base_group = np.reshape(model_grid, (-1, model_grid.shape[-1]))
                model_groups = []
                for goffset in range(0, world_size, model_parallel_size):
                    model_groups += sorted((goffset + base_group).tolist())

                if verbose and world_rank == 0:
                    print(f"Creating comm groups for id {mpname}: {model_groups}")

                for grp in model_groups:
                    if len(grp) > 1:
                        tmp_group = dist.new_group(ranks=grp)
                        if world_rank in grp:
                            _COMM_LIST.append(tmp_group)
                            _COMM_NAMES[mpname] = comm_count
                            comm_count += 1
                ranks_lookup[mpname] = model_groups

                # go for the next step
                model_grid = np.transpose(model_grid, perm)

            # now, we create a single communicator for h and w ranks
            if (get_size("h") == 1) and (get_size("w") > 1):
                if verbose and world_rank == 0:
                    print(f'Creating comm groups for id spatial: {ranks_lookup["w"]}')
                _COMM_LIST.append(get_group("w"))
                _COMM_NAMES["spatial"] = comm_count
                comm_count += 1
            elif (get_size("h") > 1) and (get_size("w") == 1):
                if verbose and world_rank == 0:
                    print(f'Creating comm groups for id spatial: {ranks_lookup["h"]}')
                _COMM_LIST.append(get_group("h"))
                _COMM_NAMES["spatial"] = comm_count
                comm_count += 1
            elif (get_size("h") > 1) and (get_size("w") > 1):
                # fuse the lists:
                def merge_ranks(list1, list2):
                    """Merge ranks"""
                    coll = list1 + list2
                    pooled = [set(subList) for subList in coll]
                    merging = True
                    while merging:
                        merging = False
                        for i, group in enumerate(pooled):
                            merged = next(
                                (g for g in pooled[i + 1 :] if g.intersection(group)), None
                            )
                            if not merged:
                                continue
                            group.update(merged)
                            pooled.remove(merged)
                            merging = True
                    return [list(x) for x in pooled]

                model_groups = merge_ranks(ranks_lookup["h"], ranks_lookup["w"])
                if verbose and world_rank == 0:
                    print(f"Creating comm groups for id spatial: {model_groups}")
                for grp in model_groups:
                    tmp_group = dist.new_group(ranks=grp)
                    if world_rank in grp:
                        _COMM_LIST.append(tmp_group)
                        _COMM_NAMES["spatial"] = comm_count
                        comm_count += 1

            # now the data and model comm:
            model_groups = np.reshape(
                np.arange(0, world_size), (-1, model_parallel_size)
            ).tolist()
            for grp in model_groups:
                if len(grp) > 1:
                    tmp_group = dist.new_group(ranks=grp)
                    if world_rank in grp:
                        _COMM_LIST.append(tmp_group)
                        _COMM_NAMES["model"] = comm_count
                        comm_count += 1

            if data_parallel_size == world_size:
                if verbose and world_rank == 0:
                    print(
                        f"Creating comm groups for id data: {[list(range(0, world_size))]}"
                    )

                _COMM_LIST.append(None)
                _COMM_NAMES["data"] = comm_count
            else:
                data_groups = [sorted(list(i)) for i in zip(*model_groups)]

                if verbose and world_rank == 0:
                    print(f"Creating comm groups for id data: {data_groups}")

                for grp in data_groups:
                    tmp_group = dist.new_group(ranks=grp)
                    if world_rank in grp:
                        _COMM_LIST.append(tmp_group)
                        _COMM_NAMES["data"] = comm_count

        # barrier
        if dist.is_initialized():
            dist.barrier(device_ids=[local_rank])

        if params.log_to_screen:
            logging.info("Finished Wireup")

        return